{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GE 254 - Intro to Geomorph\n",
    "## HW 1 - Using Python to calculate Flood Frequency Analysis\n",
    "\n",
    "#### Due: Thursday Sept. 24th by 11 am\n",
    "\n",
    "For this assignment, we will be taking the work we did in excel on flood frequency and instead creating a python script to do the same thing. You will be given a partially done code. You need to finish the code. Answer any questions shown as comments in the code. And most importantly, comment the code extensively. The end result should be a useful python script that takes any USGS flow data and calculates predictive flood frequency analysis. \n",
    "\n",
    "This exercise will provide some experience with methods used for predicting flood frequency and magnitude. We will be using the US Geological Survey (USGS) website to retrieve historical stream gauge data of the sort used to predict the likelihood of flood events of particular magnitudes during a given time interval. Such predictions are the basis for numerous engineering, restoration and development projects in and around rivers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objectives:\n",
    "    1. Practice using Python Pandas DataFrames to import, manipulate, and visualize data\n",
    "    2. Learn some powerful tools for subsetting your dataframes\n",
    "    3. Practice visualizing data \n",
    "    \n",
    "* Dr A. C. Ortiz, Sept. 2020\n",
    "* Homework assignment adapted from T. Perron Lab on flood frequency\n",
    "* Used in a 200 level undergraduate geomorphology course, after doing the same work in lab using excel (or google sheets) to calculate flood frequency and rating curves. \n",
    "* This homework assumes a basic introduction to python, pandas, and dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "#let's use pandas dataframes again\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#so I've imported 3 libraries using common \n",
    "#abbreviations to reference these libraries in my code. This is personal choice.\n",
    "#If I just wrote \"import numpy\", it would still work but when I call a numpy command\n",
    "#I would have to write numpy.matrix() vs. np.matrix() - it's all about laziness\n",
    "#and minimizing typing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make sure you've downloaded the correct data. I want you to download peak flow and daily flow measurements for 1 of these four sites (you will be assigned in class): \n",
    "* 02086849 \n",
    "* 02081000\n",
    "* 02081747  \n",
    "* 02083000 \n",
    "\n",
    "I also want you to download daily data for your site, on the waterdata.usgs.gov/nwis/sw site click on daily data instead of peak-flow data. \n",
    "\n",
    "Remember to download all available data for discharge and gage height (if available). Remember to select tab-separated file then download that file as a .txt, by right clicking on the page and selecting \"save as\".  Make sure you upload these files into JupyterHub. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading in the peak-flow measurements - this is similar to what we did in Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok let's break down this command - can you tell me what header does?\n",
    "#what is the delimiter mean?\n",
    "# why did I write skiprows?\n",
    "#what is usecols do?\n",
    "#for help with this use google - or go to the help menu above and select pandas\n",
    "\n",
    "#don't forget to add your file name to ADD_FILE_NAME_HERE.txt part of the '' in the line below\n",
    "peak_all = pd.read_csv('ADD_FILE_NAME_HERE.txt',header=72,delimiter=\"\\t\",\\\n",
    "                       skiprows=[73],usecols=range(0,8),parse_dates=True)\n",
    "peak_all.head()\n",
    "\n",
    "#if for some reason when peak_all is displayed below and you see weird values\n",
    "#in the first couple of rows, you need to adjust the values given to header\n",
    "#and skiprows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's rename these columns to something a bit more useful\n",
    "new_column_names = ['Agency', 'SiteNo', 'Date', 'Time', \\\n",
    "                    'Discharge (cfs)', 'Discharge_quality','Gage_ht (ft)',\\\n",
    "                    'Gage_quality']\n",
    "#ok now what does the \"\\\" do in the above line of code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_all.columns = new_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for some data cleanup\n",
    "peak_all['Discharge (m3/sec)'] = peak_all['Discharge (cfs)'] * 0.028316847\n",
    "peak_all['Gage_ht (m)'] = peak_all['Gage_ht (ft)'] * 3.28084\n",
    "new_station_name = \"0\" + str(peak_all['SiteNo'].unique()[0])\n",
    "peak_all['SiteNo'] = new_station_name\n",
    "\n",
    "#ok what happens in this cell?\n",
    "peak_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the fun date-time work\n",
    "peak_all['Date'] = pd.to_datetime(peak_all.Date)\n",
    "peak_all['Year'] = peak_all['Date'].dt.year\n",
    "peak_all.head()\n",
    "#explain what the above lines of code do please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok now that we've done some data management, let's pull out only the data we need\n",
    "peak = peak_all[['Year','Discharge (m3/sec)','Gage_ht (m)']]\n",
    "\n",
    "#now let's remove NaN  measurements\n",
    "peak = peak.dropna()\n",
    "print(peak.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak = peak.sort_values('Discharge (m3/sec)',ascending=False) #what does this do?\n",
    "peak.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak['Rank'] = range(1,peak.shape[0]+1) #what is the range command?\n",
    "n = peak.shape[0] #what value(s) does shape give us? why do I index at first position (0)?\n",
    "print('The total number of observations:', n)\n",
    "print(peak.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### helpful description of indexing dataframes in python & pandas\n",
    "*https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok now create a new column in peak dataframe that is the Recurrence Interval (RI = (1+n)/rank)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual procedure is to assume that the frequency distribution of floods in our record conforms to a known distribution, and plot the observed floods in such a way that they will fall along a straight line if the data conform to the assumed distribution. Distributions in common use include the Gumbel, Weibull, and Pearson Type III distributions, all of which are designed to characterize extreme value phenomena. To keep our analysis simple, we will assume that flood frequency is lognormally distributed. This implies that there should be a semilogarithmic relationship between flood magnitude and RI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok now to plot data\n",
    "peak.plot(x='Recurrence Interval (years)',y='Discharge (m3/sec)',\\\n",
    "          title='Flood Frequency of Station ' + peak_all['SiteNo'][0], \\\n",
    "          kind='scatter',logx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the trendline to our floods\n",
    "x = peak['Recurrence Interval (years)']\n",
    "y = peak['Discharge (m3/sec)']\n",
    "f = np.polyfit(np.log10(x),y,1,w=np.sqrt(y))\n",
    "print(f)\n",
    "xf = [min(x),max(x)]\n",
    "yf = f[0]*np.log10(xf) + f[1]\n",
    "print(xf,yf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now plot the data and the trendline\n",
    "\n",
    "#hint to add the trendline - use plt.plot(x,y) where you need to figure out what x and y should be!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the 100 year flood discharge? \n",
    "dis100 = \n",
    "print('the 100 year discharge is:', dis100, 'm3/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets look at the rating curve - aka stage vs. discharge\n",
    "\n",
    "\n",
    "#can you fit a line to this plot? For predicting discharge for a given stage?\n",
    "\n",
    "#Hint use polyfit again\n",
    "\n",
    "#now plot the linear trendline on the rating curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is the predicted stage for the 100 year flood?\n",
    "#stg100 = \n",
    "print('the 100 year flood stage is:', stg100, 'm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ok now that we've recreated flood recurrence interval analysis, what would you have to change to run this code on a different USGS station? \n",
    "2. What parameters might be different? What would you expect to be the same? \n",
    "3. How easy would it be to change these things?\n",
    "\n",
    "## visualizing and analyzing daily flow measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alright nice job - now let's look at daily measurements. Make sure to fill in the correct file name in the ''\n",
    "#make sure you've uploaded the .txt file to jupyter hub\n",
    "daily_all = pd.read_csv('',header=32,delimiter=\"\\t\",\\\n",
    "                        skiprows=[33],usecols=[0,1,2,3,9],parse_dates=True)\n",
    "daily_all.head()\n",
    "#again - note the header and skip-row number, these might need to change\n",
    "#for your file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = ['Agency', 'SiteNo', 'OldDateTime', \\\n",
    "                    'Discharge (cfs)', 'Gage_ht (ft)']\n",
    "daily_all.columns = new_column_names\n",
    "daily_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for some data cleanup\n",
    "\n",
    "#first change all units for gage height & discharge to mks\n",
    "\n",
    "\n",
    "#then add in the corrected station number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions about the above code:\n",
    "1. Why do I keep printing out/displaying daily_all or peak or peak_all? \n",
    "2. What is the use or reason for this in the code? \n",
    "3. What are other methods you can use to check your code validity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now calculate the datetime objects\n",
    "\n",
    "\n",
    "#now add in separate columns for day, month, and year - year is shown below, do the other two\n",
    "daily_all['Year'] = daily_all['DateTime'].dt.year\n",
    "\n",
    "\n",
    "\n",
    "print(daily_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok how can you now find the average flow for YOUR birthday? change the code as needed\n",
    "avgdis_bday = daily_all['Discharge (m3/sec)'][((daily_all.Month==7)&(daily_all.Day==7))].mean()\n",
    "\n",
    "print('The average discharge for 7/7 is: ', avgdis_bday, 'm3/sec')\n",
    "#go ahead and redo this to find the minimum, maximum, and mean flow for YOUR birthday\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now go ahead and plot the daily average flow (aka average per day)\n",
    "#this is how we use the very useful function group by\n",
    "\n",
    "avg_daily = daily_all.groupby(['Month','Day'],as_index=False).mean()\n",
    "#ok how does the above line of code work? Look up groupby and explain what is done.\n",
    "\n",
    "avg_daily.Year = 2000 #random year chosen - must be a leap year for datetime to work!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_daily['Date'] = pd.to_datetime(avg_daily[['Year','Month','Day']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now go ahead and plot your avg daily value\n",
    "avg_daily.plot(x='Date',y='Discharge (m3/sec)',\\\n",
    "               title='Averaged Daily Flow of Station ' + peak_all['SiteNo'][0], \\\n",
    "              kind='scatter')\n",
    "\n",
    "#now can you plot a monthly mean? Let's use the groupby again\n",
    "avg_m = daily_all.groupby(['Month'],as_index=False).mean()\n",
    "avg_m['Day'] = 15 #add in a fake day\n",
    "avg_m['Year'] = 2000 #add in a fake year that fits the data - make sure it matches above\n",
    "avg_m['Date'] = pd.to_datetime(avg_m[['Year','Month','Day']])\n",
    "print(avg_m)\n",
    "plt.plot(avg_m.Date,avg_m['Discharge (m3/sec)'],'--m',linewidth=4)\n",
    "\n",
    "#does the monthly average match the daily averages values - are the trends holding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(daily_all.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling Data\n",
    "Now let's calculate the per month average flow over our timeseries (aka resample our data)\n",
    "look at this or some helpful information *https://sergilehkyi.com/tips-on-working-with-datetime-index-in-pandas/* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_all2 = daily_all.set_index('DateTime')\n",
    "print(daily_all2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = daily_all2.resample('M').mean() #look up what this command does - this can be very powerful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ma.head())\n",
    "print(daily_all2.head()) \n",
    "#explain the two new dataframes created (ma and daily_all2). What information is in these?\n",
    "#why did I create these? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_all.plot(x='DateTime', y='Discharge (m3/sec)',kind='scatter')\n",
    "plt.plot(ma.index,ma['Discharge (m3/sec)'],\"--k\",linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "1. What do you see in the above plots? \n",
    "2. What error might be found by the order in which we analyzed data?\n",
    "3. Looking at this plot, do you think you might switch the order of operations of this homework? \n",
    "4. Would you maybe analyze or even visualize one piece of data before another? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok last task:\n",
    "    1. Plot the rating curve based on the daily data (see daily_all)\n",
    "    2. On top of that, plot the rating curve based on the monthly averaged data (see ma dataframe)\n",
    "    3. Finally plot on top of that, the rating curve based on peak yearly flows (see peak)\n",
    "    4. What differences occur in these different datasets? Make sure you plot each dataset with a different color and/or marker to easily highlight the differences. Make sure to add a legend! \n",
    "\n",
    "Ok now fit a linear trendlines to each dataset (you should have 3 in total, and remember, you've already calculated one of these trendlines earlier on). \n",
    "* How do these trendlines differ? What might be driving these differences? \n",
    "* What errors might be caused by using one dataset over the other? \n",
    "* Which dataset would you use to most accurately predict discharge from the stage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now send me a copy of your .ipynb file and the two .txt files you've analyzed in this code. Please remember to give it a unique name to the file name (aka add your initials). **Nice job with Homework 1!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
